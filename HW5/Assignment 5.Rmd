---
title: "PM 591 -- Machine Learning for the Health Sciences."
author: "Haoran Zhang"
date: "Due 4/18/2022"
output: 
  html_document:
    keep_md: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


### Exercise 1 (Analysis/conceptual)
You will assess how well a tree model can capture non-linearities by fitting a regression tree to simulated non-linear data.

i. Simulate the data

```{r}
library(mlr)
library(randomForest)
library(rpart)
library(rpart.plot)

set.seed(1984) 
n = 1000
x = runif(n, -5, 5) # n observations uniformly distributed in the interval -5 to 5
error = rnorm(n, sd=0.5)
y = sin(x) + error # nonlinear relationship between outcome y and feature x
nonlin = data.frame(y=y, x=x)
```

ii. Split the data into training and testing (500 observations in each). Plot the data -- scatterplot of y vs. x
```{r}
train = sample(1:nrow(nonlin), floor(nrow(nonlin)*0.5)); test = setdiff(1:nrow(nonlin), train)
```

iii. Fit a regression tree using the trainig set 

```{r}
library(rpart)
treefit = rpart(y~x, method='anova', control=list(cp=0), data=nonlin[train,]) # Method='anova' indicate sregression tree. cp=0 ensures that binary recursive partitioning will not stop early due to lack of improvement in RSS by an amount  of at least cp
```

iv. Plot the fitted regression tree

```{r}
plot(treefit) # plots the tree
text(treefit) # annotates the tree. May fail if tree is too large

library(rpart.plot)
rpart.plot(treefit) #the rpart.plot function generates better looking trees!
```

Note: the height of the branches are proportional to the improvement in RSS

v. Plot the cv relative error to determine the optimal complexity parameter

```{r}
plotcp(treefit)
```

vi. Print the table complexity parameter values and their associated cv-errors

```{r}
printcp(treefit)
```

vii. Select the optimal complexity parameter and prune the tree

```{r}
optimalcp = treefit$cptable[which.min(treefit$cptable[,"xerror"]),"CP"] # for you to fill in
treepruned = prune(treefit, cp=optimalcp)
```

viii. Plot the pruned tree
```{r}
rpart.plot(treepruned)
```

ix. Summarize the pruned tree object and relate the summary to the plotted tree above

```{r}
summary(treepruned)
```

x. Based on the plot and/or summary of the pruned tree create a vector of the (ordered) split points for variable x, and a vector of fitted values for the intervals determined by the split points of x.

```{r}
# for you to fill in
x_splits = c(-4.6,-3.5,-2.96,-2.1,-0.89,-0.055,0.48,0.97,2.5,3.1,3.8)

y_splits = c(1.2, 0.78,0.034,-0.67,-1.1,-0.52,-0.0058,0.58,1,0.63,-0.31,-0.93)
```

xi. Plot the step function corresponding to the fitted (pruned) tree

```{r, eval=FALSE}
plot(y~x, data=nonlin[train,])
stpfn = stepfun(x_splits, y_splits) #stepfun creates the step function 
plot(stpfn, add=TRUE, lwd=2, col='red4') #add=TRUE plots over the existing plot 
```

xii. Fit a linear model to the training data and plot the regression line. Contrats the quality of the fit of the tree model vs. linear regression by inspection of the plot

```{r}
lmfit = lm(y ~ x, data=nonlin[train,])
summary(lmfit)
plot(lmfit)
abline(lmfit, col='blue', lwd=2)
```
 
xiii. Compute the test MSE of the pruned tree and the linear regression model


### Exercise 2 (Analysis)
You will recreate the analysis of the heart data in the textbook and lecture. 
```{r}
heart <- read.csv("Heart.csv")
require(mlr)
require(randomForest)
require(rpart)
require(rpart.plot)

set.seed(301)
heart <- heart[complete.cases(heart), ]

heart$ChestPain <- as.factor(heart$ChestPain)
heart$Thal <- as.factor(heart$Thal)
heart$AHD <- as.factor(heart$AHD)
heart$AHD <- as.numeric(heart$AHD)-1
```

i.   Split the data into training and testing
```{r}
nh <- nrow(heart)
train_heart = sample(1:nrow(heart), floor(nrow(heart)*0.7)); test_heart = setdiff(1:nrow(heart), train)
```

ii.  Fit a classification tree using ``rpart``
```{r}
set.seed(301)
treefit_heart = rpart(AHD~., method='class', control=list(cp=0), data=heart[train_heart,]) 
```

iii. Plot the unpruned tree
```{r}
rpart.plot(treefit_heart)
```

iv.  Plot the cv error
```{r}
min(treefit_heart$cptable[,"xerror"])*nrow(heart[train_heart,])

plotcp(treefit_heart)
```

v. Prune the tree using the optimal complexity parameter
```{r}
optimalcp_heart = treefit_heart$cptable[which.min(treefit_heart$cptable[,"xerror"]),"CP"] 
treepruned_heart = prune(treefit_heart, cp=optimalcp_heart)
treepruned_heart$cptable
```

vi. Plot the pruned tree
```{r}
rpart.plot(treepruned_heart)
```

vii. Compute the test misclassification error

```{r}
treepruned_heart_predict = predict(treepruned_heart, newdata =heart[test_heart,], type='prob')
```

```{r}
library(pROC)
roc_test_heart = roc(heart$AHD[test_heart], treepruned_heart_predict[,1])
print(auc(roc_test_heart)) 
print(ci.auc(roc_test_heart))
cm <- table(heart$AHD[test_heart], as.numeric(treepruned_heart_predict[,1]>0.5))
    mmce <- 1 - (sum(diag(cm))/sum(cm))
    print(mmce)
```
vii. Fit the tree with the optimal complexity parameter to the full data (training + testing)
```{r}
set.seed(301)
heart_full = rpart(AHD~., method='class', control=list(cp=0), data=heart) 
rpart.plot(heart_full)
optimalcp_full = heart_full$cptable[which.min(heart_full$cptable[,"xerror"]),"CP"] 
optimalcp_full
treepruned_full = prune(heart_full, cp=optimalcp_full)
treepruned_full$cptable
rpart.plot(treepruned_full)
heart_full$cptable
# cross validation error 
min(heart_full$cptable[,"xerror"])*nrow(heart)
```

### Exercise 3 -- Analysis
Compare the performance of classification trees, bagging, random forests, and boosting for predicting heart disease based on the ``heart`` data. 

i. Split the data into training and testing. Train each of the models on the training data and extract the cross-validation (or out-of-bag error for bagging and Random forest). 
```{r}
train = sample(1:nrow(heart), floor(nrow(heart)*0.7)); test = setdiff(1:nrow(heart), train)
```

   a. For classification trees use ``rpart`` with pruning. Plot the tree using ``fancyRpartPlot`` in package ``rattle``. Plot the variable importance .
```{r}
library(rattle)
heart_cl = rpart(AHD ~ ., data=heart, method='class',
 control=list(minsplit = 15, minbucket = 5, cp = 0))

heart_cl_pruned = prune(heart_cl, cp=0.032)
fancyRpartPlot(heart_cl_pruned, cex=0.6, yesno=2)
```

   b. For bagging use ``randomForest`` with ``mtry`` equal to the number of features (all other parameters at their default values). Generate the variable importance plot using ``varImpPlot`` and extract variable importance from the ``randomForest`` fitted object using the ``importance`` function.
```{r}
set.seed(301)
heart_bg = randomForest(as.factor(AHD) ~ . , data = heart[train,],
                        strata = heart$AHD[train],
                        sampsize = as.vector(table(heart$AHD[train]))) 
#oob error rate
sum(heart_bg$err.rate[,1])


varImpPlot(heart_bg, cex = 0.7, pt.cex = 1.2, n.var = 20, main = "", pch = 16,
 col = "red4")

#varimp = round(summary(heart_tree_pruned)$variable.importance, 2)

```

   c.  For random forests use ``randomForest`` with the default parameters. Generate the variable importance plot using ``varImpPlot`` and extract variable importance from the ``randomForest`` fitted object using the ``importance`` function. 
```{r}
heart_rf = randomForest(as.factor(AHD) ~ . , data = heart[train,],
                        mtry=dim(heart[train,])[2]-1,
                        strata = heart$AHD[train],
                        sampsize = as.vector(table(heart$AHD[train]))) 
#oob error rate
sum(heart_rf$err.rate[,1])

varImpPlot(heart_rf, cex = 0.7, pt.cex = 1.2, n.var = 20, main = "", pch = 16,
 col = "red4")

#important.par<-importance(heart_rf)[order(importance(heart_rf)[,1], decreasing = TRUE),]
```

   d. For boosting use `gbm` with ``cv.folds=5`` to perform 5-fold cross-validation, and set ``class.stratify.cv`` to ``AHD`` (heart disease outcome) so that cross-validation is performed stratifying by ``AHD``.  Plot the cross-validation error as a function of the boosting iteration/trees (the `$cv.error` component of the object returned by ``gbm``) and determine whether additional boosting iterations are warranted. If so, run additional iterations with  ``gbm.more`` (use the R help to check its syntax). Choose the optimal number of iterations. Use the ``summary.gbm`` function to generate the variable importance plot and extract variable importance/influence (``summary.gbm`` does both). Generate 1D and 2D marginal plots with ``gbm.plot`` to assess the effect of the top three variables and their 2-way interactions. 
```{r}
library(gbm, warn.conflicts = FALSE)

n <- nrow(heart)
train = sample(1:nrow(heart), floor(nrow(heart)*0.7)); test = setdiff(1:nrow(heart), train)

set.seed(301)
heart_boost = gbm(AHD ~ . , data=heart[train,], 
                    distribution='bernoulli', n.trees=3000,
                    interaction.depth = 1,
                    shrinkage = 0.01,
                    cv.folds=5, class.stratify.cv=TRUE)

plot(heart_boost$train.error, cex.lab=2, cex.axis=2, col='red4', type='l', lwd=3, 
         ylim=c(0,1.5), ylab="error")
lines(heart_boost$cv.error, col='steelblue', lwd=3)

```

   
 e.  Compute the AUC for the 4 methods and comment on their relative performance.t missclassification error for the 4 methods and comment on their relative performance.
```{r}
print("===random forest===") 
heart_rf_predict = predict(heart_rf, newdata = heart[test, ], 
                              type='response')

roc_test = roc(heart$AHD[test], as.numeric(heart_rf_predict)-1) 
auc(roc_test); ci.auc(roc_test)
cm <- table(heart$AHD[test], as.numeric(heart_rf_predict)-1)
mmce <- 1 - (sum(diag(cm))/sum(cm))
mmce
```


```{r}
print("===bagging===") 
heart_bg_predict = predict(heart_bg, newdata = heart[test, ], 
                              type='response')

roc_test = roc(heart$AHD[test], as.numeric(heart_bg_predict)-1) 
auc(roc_test); ci.auc(roc_test)
cm <- table(heart$AHD[test], as.numeric(heart_bg_predict)-1)
mmce <- 1 - (sum(diag(cm))/sum(cm))
mmce

print("===boost===") 
heart_boost_predict = predict(heart_boost, newdata = heart[test, ], 
                              type='response', n.trees=which.min(heart_boost$cv.error))


roc_test = roc(heart$AHD[test], as.numeric(heart_boost_predict>0.5)) 
auc(roc_test); ci.auc(roc_test)
cm <- table(heart$AHD[test], as.numeric(heart_boost_predict>0.5))
mmce <- 1 - (sum(diag(cm))/sum(cm))
mmce
```

ii. (Extra Credit) Perform the comparison between methods in i) using mlr3 benchmark and add to the mix of methods the boosting implemented in xgboost 


### Exercise 3 -- Analysis/conceptual 
Yo will evaluate the effect of critical boosting parameters (number of boosting iterations, shrinkage/learning rate, and tree depth/interaction) on the Metabric data.  In ``gbm`` the number of iterations is controlled by ``n.trees`` (default is 100), the shrinkage/learning rate is controlled by ``shrinkage`` (default is 0.001), and interaction depth by ``interaction.depth`` (default is 1).

i. Split the metabric data into training and testing. 
```{r, tidy=TRUE, tidy.opts=list(width.cutoff=65)}
load("metabric.Rdata")
set.seed(2022)
train = sample(1:nrow(metabric), floor(nrow(metabric)*0.7)); 
test = setdiff(1:nrow(metabric), train)
table(metabric$y[train])
metabric$y <- as.numeric(metabric$y)-1

```
ii. Set the seed and train a boosting classification with ``gbm`` using 10-fold cross-validation (``cv.folds=10``) on the training data with ``n.trees = 5,000``, ``shrinkage = 0.001``, and ``interaction.depth =1``. Plot the cross-validation errors as a function of the boosting iteration.
```{r}
library(gbm, warn.conflicts = FALSE)
set.seed(2022)
   metabric_boost= gbm(y ~ . , data=metabric[train,], 
                    distribution='bernoulli', n.trees=5000,
                    interaction.depth = 1,
                    shrinkage = 0.001,
                    cv.folds=10, class.stratify.cv=TRUE)
```

```{r}
plot(metabric_boost$train.error, cex.lab=2, cex.axis=2, col='red4', type='l', lwd=3,  xlim=c(0,5000),
         ylim=c(0,1.5), ylab="error", main= paste0("cv.error = ",round(min(metabric_boost$cv.error),4),
                                                   "; n.trees = ", 5000,
                                                   "\ndepth = ", 1, "; shrinkage = ", 0.001))
lines(metabric_boost$cv.error, col='steelblue', lwd=3)
```

```{r}
metabric_boost_predict = predict(metabric_boost, newdata = metabric[test, ], type='response')
```

```{r}
require(pROC)
roc_test = roc(metabric$y[test], metabric_boost_predict) 
    print(auc(roc_test))
    print(ci.auc(roc_test))
    cm <- table(metabric$y[test], as.numeric(metabric_boost_predict>0.5))
    mmce <- 1 - (sum(diag(cm))/sum(cm))
    print(mmce)
```

```{r}
library(pROC)
##a.a
as.data.frame(head(summary(metabric_boost, plotit = FALSE), 3))
pa1 = plot(metabric_boost, i.var=c('ILMN_1770085'), type='response', return.grid=TRUE)
plot(pa1, type ='l', lwd=3, col='red4', cex.axis=1, cex.lab=1)
```
iii. Repeat ii. using the same seed and ``n.trees=5,000`` with the following 3 additional combination of parameters: a) ``shrinkage = 0.001``, ``interaction.depth = 2``; b) ``shrinkage = 0.01``, ``interaction.depth = 1``; c) ``shrinkage = 0.01``, ``interaction.depth = 2``.
```{r}
library(gbm, warn.conflicts = FALSE)
set.seed(2022)
   metabric_boost_a = gbm(y ~ . , data=metabric[train,], 
                    distribution='bernoulli', n.trees=5000,
                    interaction.depth = 2,
                    shrinkage = 0.001,
                    cv.folds=10, class.stratify.cv=TRUE)
   metabric_boost_b = gbm(y ~ . , data=metabric[train,], 
                    distribution='bernoulli', n.trees=5000,
                    interaction.depth = 1,
                    shrinkage = 0.01,
                    cv.folds=10, class.stratify.cv=TRUE)
   metabric_boost_c = gbm(y ~ . , data=metabric[train,], 
                    distribution='bernoulli', n.trees=5000,
                    interaction.depth = 2,
                    shrinkage = 0.01,
                    cv.folds=10, class.stratify.cv=TRUE)
```

```{r}
plot(metabric_boost_a$train.error, cex.lab=2, cex.axis=2, col='red4', type='l', lwd=3,  xlim=c(0,5000),
         ylim=c(0,1.5), ylab="error", main= paste0("cv.error = ",round(min(metabric_boost_a$cv.error),4),
                                                   "; n.trees = ", 5000,
                                                   "\ndepth = ", 2, "; shrinkage = ", 0.001))
lines(metabric_boost_a$cv.error, col='steelblue', lwd=3)
```

```{r}
metabric_boost_a_predict = predict(metabric_boost_a, newdata = metabric[test, ], type='response')
```

```{r}
require(pROC)
roc_test_a = roc(metabric$y[test], metabric_boost_a_predict) 
    print(auc(roc_test_a))
    print(ci.auc(roc_test_a))
    cm <- table(metabric$y[test], as.numeric(metabric_boost_a_predict>0.5))
    mmce <- 1 - (sum(diag(cm))/sum(cm))
    print(mmce)
```

```{r}
library(pROC)
##a.a
as.data.frame(head(summary(metabric_boost_a, plotit = FALSE), 3))
pa1_a = plot(metabric_boost, i.var=c('ILMN_1770085'), type='response', return.grid=TRUE)
plot(pa1, type ='l', lwd=3, col='red4', cex.axis=1, cex.lab=1)
```


```{r}
plot(metabric_boost_b$train.error, cex.lab=2, cex.axis=2, col='red4', type='l', lwd=3,  xlim=c(0,5000),
         ylim=c(0,1.5), ylab="error", main= paste0("cv.error = ",round(min(metabric_boost_b$cv.error),4),
                                                   "; n.trees = ", 5000,
                                                   "\ndepth = ", 1, "; shrinkage = ", 0.01))
lines(metabric_boost_b$cv.error, col='steelblue', lwd=3)
```

```{r}
metabric_boost_b_predict = predict(metabric_boost_b, newdata = metabric[test, ], type='response')
```

```{r}
require(pROC)
roc_test_b = roc(metabric$y[test], metabric_boost_b_predict) 
    print(auc(roc_test_b))
    print(ci.auc(roc_test_b))
    cm <- table(metabric$y[test], as.numeric(metabric_boost_b_predict>0.5))
    mmce <- 1 - (sum(diag(cm))/sum(cm))
    print(mmce)
```

```{r}
library(pROC)
##a.a
as.data.frame(head(summary(metabric_boost_b, plotit = FALSE), 3))
pa1_b = plot(metabric_boost_b, i.var=c('ILMN_1770085'), type='response', return.grid=TRUE)
plot(pa1_b, type ='l', lwd=3, col='red4', cex.axis=1, cex.lab=1)
```


```{r}
plot(metabric_boost_c$train.error, cex.lab=2, cex.axis=2, col='red4', type='l', lwd=3,  xlim=c(0,5000),
         ylim=c(0,1.5), ylab="error", main= paste0("cv.error = ",round(min(metabric_boost_c$cv.error),4),
                                                   "; n.trees = ", 5000,
                                                   "\ndepth = ", 2, "; shrinkage = ", 0.01))
lines(metabric_boost_c$cv.error, col='steelblue', lwd=3)
```

```{r}
metabric_boost_c_predict = predict(metabric_boost_c, newdata = metabric[test, ], type='response')
```

```{r}
require(pROC)
roc_test_c = roc(metabric$y[test], metabric_boost_c_predict) 
    print(auc(roc_test_c))
    print(ci.auc(roc_test_c))
    cm <- table(metabric$y[test], as.numeric(metabric_boost_c_predict>0.5))
    mmce <- 1 - (sum(diag(cm))/sum(cm))
    print(mmce)
```

```{r}
library(pROC)
##a.a
as.data.frame(head(summary(metabric_boost_c, plotit = FALSE), 3))
pa1_c = plot(metabric_boost_c, i.var=c('ILMN_1770085'), type='response', return.grid=TRUE)
plot(pa1_c, type ='l', lwd=3, col='red4', cex.axis=1, cex.lab=1)
```



iii. Choose the best parameter combination among the ones examinded above to a) generate 1D and 2D marginal plots with ``gbm.plot`` to assess the effect of the top three variables and their 2-way interactions; b) compute the test missclassification error and AUC.

Combination b has the largest auc 0.6816.

```{r}
metabric_boost_b_predict = predict(metabric_boost_b, newdata = metabric[test, ], type='response')
```

```{r}
require(pROC)
roc_test_b = roc(metabric$y[test], metabric_boost_b_predict) 
    print(auc(roc_test_b))
    print(ci.auc(roc_test_b))
    cm <- table(metabric$y[test], as.numeric(metabric_boost_b_predict>0.5))
    mmce <- 1 - (sum(diag(cm))/sum(cm))
    print(mmce)
```

```{r}
library(pROC)
##a.a
as.data.frame(head(summary(metabric_boost_b, plotit = FALSE), 3))
pa1_b = plot(metabric_boost_b, i.var=c('ILMN_1770085'), type='response', return.grid=TRUE)
plot(pa1_b, type ='l', lwd=3, col='red4', cex.axis=1, cex.lab=1)
```


