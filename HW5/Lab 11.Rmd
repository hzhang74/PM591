---
title: "PM 591 -- Machine Learning for the Health Sciences."
author: "Assignment 6"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
library(knitr, warn.conflicts = FALSE)
knitr::opts_chunk$set(echo = TRUE)
opts_knit$set(root.dir='/Users/Kaili/Downloads')

```

<br>

### Exercise 1 -- Analysis
Compare the performance of classification trees, bagging, random forests, and boosting for predicting heart disease based on the ``heart`` data.

i. Split the data into training and testing. Train each of the models on the training data and extract the cross-validation (or out-of-bag error for bagging and Random forest). 

```{r, tidy=TRUE, tidy.opts=list(width.cutoff=65), fig.width=5, fig.height=3}
heart <- read.csv("Heart.csv")
require(mlr)
require(randomForest)
require(rpart)
require(rpart.plot)

set.seed(301)
heart <- heart[complete.cases(heart), ]

heart$ChestPain <- as.factor(heart$ChestPain)
heart$Thal <- as.factor(heart$Thal)
heart$AHD <- as.factor(heart$AHD)
heart$AHD <- as.numeric(heart$AHD)-1
```


```{r, tidy=TRUE, tidy.opts=list(width.cutoff=65), fig.width=5, fig.height=3}
#(i) split data
n <- nrow(heart)
train = sample(1:nrow(heart), floor(nrow(heart)*0.7)); test = setdiff(1:nrow(heart), train)
table(heart$AHD[train])
```
- We impletemented the missing data using the randomForest package. 
- 96 patients have the outcome while 116 patients do not have the oucome.

```{r, tidy=TRUE, tidy.opts=list(width.cutoff=65), fig.width=5, fig.height=3}
set.seed(301)
treefit = rpart(AHD~., method='class', control=list(cp=0), data=heart[train,]) 
# Method='anova' indicate sregression tree. cp=0 ensures that binary recursive partitioning will not stop early due to lack of improvement in RSS by an amount  of at least cp
optimalcp = treefit$cptable[which.min(treefit$cptable[,"xerror"]),"CP"]  # for you to fill in
optimalcp
treepruned = prune(treefit, cp=optimalcp)
treepruned$cptable
rpart.plot(treepruned)
treefit$cptable
# cross validation error 
min(treefit$cptable[,"xerror"])*nrow(heart[train,])
```

   a. For bagging use ``randomForest`` with ``mtry`` equal to the number of features (all other parameters at their default values). Generate the variable importance plot using ``varImpPlot`` and extract variable importance from the ``randomForest`` fitted object using the ``importance`` function.
 
```{r, tidy=TRUE, tidy.opts=list(width.cutoff=65), fig.width=5, fig.height=3}
set.seed(301)
heart_rf = randomForest(as.factor(AHD) ~ . , data = heart[train,],
                        mtry=dim(heart[train,])[2]-1,
                        strata = heart$AHD[train],
                        sampsize = as.vector(table(heart$AHD[train]))) 
#oob error rate
sum(heart_rf$err.rate[,1])

set.seed(301)
heart_bg = randomForest(as.factor(AHD) ~ . , data = heart[train,],
                        strata = heart$AHD[train],
                        sampsize = as.vector(table(heart$AHD[train]))) 
#oob error rate
sum(heart_bg$err.rate[,1])

```

   b.  For random forests use ``randomForest`` with the default parameters. Generate the variable importance plot using ``varImpPlot`` and extract variable importance from the ``randomForest`` fitted object using the ``importance`` function. 

```{r, tidy=TRUE, tidy.opts=list(width.cutoff=65)}
require(pROC,warn.conflicts = FALSE)
varImpPlot(heart_rf, cex.lab=1.5, cex.axis=2, cex=1.3, 
           n.var=20, main="", pch=16, col='red4')
 
important.par<-importance(heart_rf)[order(importance(heart_rf)[,1], decreasing = TRUE),]

``` 
   
   c. For boosting use `gbm` with ``cv.folds=5`` to perform 5-fold cross-validation, and set ``class.stratify.cv`` to ``AHD`` (heart disease outcome) so that cross-validation is performed stratifying by ``AHD``.  Plot the cross-validation error as a function of the boosting iteration/trees (the `$cv.error` component of the object returned by ``gbm``) and determine whether additional boosting iterations are warranted. If so, run additional iterations with  ``gbm.more`` (use the R help to check its sintax). Choose the optimal number of iterations. Use the ``summary.gbm`` funtion to generate the variable importance plot and extract variable importance/influence (``summary.gbm`` does both). Generate 1D and 2D marginal plots with ``gbm.plot`` to assess the effect of the top three variables and their 2-way interactions. 


```{r, tidy=TRUE, tidy.opts=list(width.cutoff=65), fig.width=4, fig.height=3}
library(gbm, warn.conflicts = FALSE)


n <- nrow(heart)
train = sample(1:nrow(heart), floor(nrow(heart)*0.7)); test = setdiff(1:nrow(heart), train)


set.seed(301)
heart_boost = gbm(AHD ~ . , data=heart[train,], 
                    distribution='bernoulli', n.trees=3000,
                    interaction.depth = 1,
                    shrinkage = 0.01,
                    cv.folds=5, class.stratify.cv=TRUE)
```


```{r, tidy=TRUE, tidy.opts=list(width.cutoff=65), fig.width=4, fig.height=3}
plot(heart_boost$train.error, cex.lab=2, cex.axis=2, col='red4', type='l', lwd=3, 
         ylim=c(0,1.5), ylab="error")
lines(heart_boost$cv.error, col='steelblue', lwd=3)

```
   
ii. Compute the test misclassification error for the 4 methods and comment on their relative performance.



```{r, tidy=TRUE, tidy.opts=list(width.cutoff=65), fig.width=5, fig.height=3, include=FALSE}
print("===random forest===") 
heart_rf_predict = predict(heart_rf, newdata = heart[test, ], 
                              type='response')

roc_test = roc(heart$AHD[test], as.numeric(heart_rf_predict)-1) 
auc(roc_test); ci.auc(roc_test)
cm <- table(heart$AHD[test], as.numeric(heart_rf_predict)-1)
mmce <- 1 - (sum(diag(cm))/sum(cm))
mmce
```


```{r, tidy=TRUE, tidy.opts=list(width.cutoff=65), fig.width=5, fig.height=3, include=FALSE}
print("===bagging===") 
heart_bg_predict = predict(heart_bg, newdata = heart[test, ], 
                              type='response')

roc_test = roc(heart$AHD[test], as.numeric(heart_bg_predict)-1) 
auc(roc_test); ci.auc(roc_test)
cm <- table(heart$AHD[test], as.numeric(heart_bg_predict)-1)
mmce <- 1 - (sum(diag(cm))/sum(cm))
mmce

print("===boost===") 
heart_boost_predict = predict(heart_boost, newdata = heart[test, ], 
                              type='response', n.trees=which.min(heart_boost$cv.error))


roc_test = roc(heart$AHD[test], as.numeric(heart_boost_predict>0.5)) 
auc(roc_test); ci.auc(roc_test)
cm <- table(heart$AHD[test], as.numeric(heart_boost_predict>0.5))
mmce <- 1 - (sum(diag(cm))/sum(cm))
mmce
```

- Among four methods, the boost method returns the hightest AUC (0.8464) and lowest test misclassification error (0.154). 


### Exercise 2 -- Analysis/conceptual 
Yo will evaluate the effect of critical boosting parameters (number of boosting iterations, shrinkage/learning rate, and tree depth/interaction) on the Metabric data.  In ``gbm`` the number of iterations is controlled by ``n.trees`` (default is 100), the shrinkage/learning rate is controlled by ``shrinkage`` (default is 0.001), and interaction depth by ``interaction.depth`` (default is 1).

i. Split the metabric data into training and testing. 
```{r, tidy=TRUE, tidy.opts=list(width.cutoff=65)}
load("metabric.Rdata")
set.seed(2022)
train = sample(1:nrow(metabric), floor(nrow(metabric)*0.7)); 
test = setdiff(1:nrow(metabric), train)
table(metabric$y[train])
metabric$y <- as.numeric(metabric$y)-1

```
ii. Set the seed and train a boosting classification with ``gbm`` using 10-fold cross-validation (``cv.folds=10``) on the training data with ``n.trees = 5,000``, ``shrinkage = 0.001``, and ``interaction.depth =1``. Plot the cross-validation errors as a function of the boosting iteration.
```{r}
library(gbm, warn.conflicts = FALSE)
set.seed(2022)
   metabric_boost= gbm(y ~ . , data=metabric[train,], 
                    distribution='bernoulli', n.trees=6500,
                    interaction.depth = 1,
                    shrinkage = 0.001,
                    cv.folds=2, class.stratify.cv=TRUE)
```


```{r}
plot(metabric_boost$train.error, cex.lab=2, cex.axis=2, col='red4', type='l', lwd=3,  xlim=c(0,6500),
         ylim=c(0,1.5), ylab="error", main= paste0("cv.error = ",round(min(metabric_boost$cv.error),4),
                                                   "; n.trees = ", 6500,
                                                   "\ndepth = ", 1, "; shrinkage = ", 0.01))
lines(metabric_boost$cv.error, col='steelblue', lwd=3)
```
```{r}
metabric_boost_predict = predict(metabric_boost, newdata = metabric[test, ], type='response')
```


```{r}
require(pROC)
roc_test = roc(metabric$y[test], metabric_boost_predict) 
    print(auc(roc_test))
    print(ci.auc(roc_test))
    cm <- table(metabric$y[test], as.numeric(metabric_boost_predict>0.5))
    mmce <- 1 - (sum(diag(cm))/sum(cm))
    print(mmce)
```

```{r}
library(pROC)
##a.a
as.data.frame(head(summary(metabric_boost, plotit = FALSE), 3))
pa1 = plot(metabric_boost, i.var=c('ILMN_1770085'), type='response', return.grid=TRUE)
plot(pa1, type ='l', lwd=3, col='red4', cex.axis=1, cex.lab=1)
```
